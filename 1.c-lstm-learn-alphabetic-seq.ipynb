{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 循環神經網絡 LSTM (長短期記憶)來學習字母表順序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很多人在看過RNN或LSTM的原理說明後, 對於RNN神經網絡在序列資料的學習與應用上很難一開始就理解。在本文中，我們將開發和比較幾種不同的LSTM神經網絡模型。\n",
    "\n",
    "![lstm-abc](https://pro.guidesocial.be/images/thumbs/580x387/arton24101.jpg?fct=1456434296)\n",
    "\n",
    "我們將要使用深度學習來學習英文26個字母出現的順序。也就是說，給定一個英文字母表的某一個字母，來讓神經網絡預測下一個可能會出現的字母。\n",
    "\n",
    "> ABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "\n",
    "> 例如: \n",
    "\n",
    "> 給 J -> 預測 K\n",
    "\n",
    "> 給 X -> 預測 Y\n",
    "\n",
    "這是一個簡單的序列預測問題，一旦被理解，就可以推廣到其他序列預測問題，如時間序列預測和序列分類。\n",
    "\n",
    "![lstm-many-to-one](https://i.stack.imgur.com/QCnpU.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型 1. 用LSTM學習一個字符到一個字符映射"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP1. 匯入 Keras 及相關模組"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 給定隨機的種子, 以便讓大家跑起來的結果是相同的\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP2. 準備資料\n",
    "\n",
    "我們現在可以定義我們的數據集，字母表(alphabet)。為了便於閱讀，我們使用大寫字母來定義字母表。\n",
    "\n",
    "我們需要將字母表的每個字母映射到數字以便使用人工網絡來進行訓練。我們可以通過為字符創建字母索引的字典來輕鬆完成此操作。\n",
    "我們還可以創建一個反向查找，將預測轉換回字符以供以後使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義序列數據集\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# 創建字符映射到整數（0 - 25)和反相的查詢字典物件\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字母對應到數字編號: \n",
      " {'O': 14, 'T': 19, 'M': 12, 'Y': 24, 'E': 4, 'I': 8, 'P': 15, 'X': 23, 'R': 17, 'V': 21, 'W': 22, 'S': 18, 'H': 7, 'Z': 25, 'D': 3, 'J': 9, 'B': 1, 'A': 0, 'Q': 16, 'C': 2, 'G': 6, 'L': 11, 'K': 10, 'F': 5, 'U': 20, 'N': 13}\n",
      "\n",
      "\n",
      "數字編號對應到字母: \n",
      " {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J', 10: 'K', 11: 'L', 12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T', 20: 'U', 21: 'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z'}\n"
     ]
    }
   ],
   "source": [
    "# 打印看一下\n",
    "print(\"字母對應到數字編號: \\n\", char_to_int)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"數字編號對應到字母: \\n\", int_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP3. 準備訓練用資料\n",
    "\n",
    "現在我們需要創建我們的輸入(X)和輸出(y)來訓練我們的神經網絡。我們可以通過定義一個輸入序列長度，然後從輸入字母序列中讀取序列。\n",
    "例如，我們使用輸入長度1.從原始輸入數據的開頭開始，我們可以讀取第一個字母“A”，下一個字母作為預測“B”。我們沿著一個字符移動並重複，直到達到“Z”的預測。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A -> B\n",
      "B -> C\n",
      "C -> D\n",
      "D -> E\n",
      "E -> F\n",
      "F -> G\n",
      "G -> H\n",
      "H -> I\n",
      "I -> J\n",
      "J -> K\n",
      "K -> L\n",
      "L -> M\n",
      "M -> N\n",
      "N -> O\n",
      "O -> P\n",
      "P -> Q\n",
      "Q -> R\n",
      "R -> S\n",
      "S -> T\n",
      "T -> U\n",
      "U -> V\n",
      "V -> W\n",
      "W -> X\n",
      "X -> Y\n",
      "Y -> Z\n"
     ]
    }
   ],
   "source": [
    "# 準備輸入數據集\n",
    "seq_length = 1\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "    seq_in = alphabet[i:i + seq_length]\n",
    "    seq_out = alphabet[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    print(seq_in, '->', seq_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP4. 資料預處理\n",
    "我們需要將NumPy數組重塑為LSTM網絡所期望的格式，也就是: (samples, time_steps, features)。\n",
    "同時我們將進行資料的歸一化(normalize)來讓資料的值落於0到1之間。並對標籤值進行one-hot的編碼。\n",
    "\n",
    "\n",
    "> ABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "\n",
    "> 例如: \n",
    "\n",
    "> 給 J -> 預測 K\n",
    "\n",
    "> 給 X -> 預測 Y\n",
    "\n",
    "\n",
    "目標訓練張量結構: (samples, time_steps, features) -> (n , **1**, **1** )\n",
    "\n",
    "請特別注意, 這裡的1個字符會變成1個時間步裡頭的1個element的\"feature\"向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (25, 1, 1)\n",
      "y shape:  (25, 26)\n"
     ]
    }
   ],
   "source": [
    "# 重塑 X 資料的維度成為 (samples, time_steps, features)\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "\n",
    "# 歸一化\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# one-hot 編碼輸出變量\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "print(\"X shape: \", X.shape) # (25筆samples, \"1\"個時間步長, 1個feature)\n",
    "print(\"y shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP5. 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 32)                4352      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 26)                858       \n",
      "=================================================================\n",
      "Total params: 5,210\n",
      "Trainable params: 5,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 創建模型\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP6. 定義訓練並進行訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " - 2s - loss: 3.2660 - acc: 0.0000e+00\n",
      "Epoch 2/500\n",
      " - 0s - loss: 3.2582 - acc: 0.0000e+00\n",
      "Epoch 3/500\n",
      " - 0s - loss: 3.2551 - acc: 0.0400\n",
      "Epoch 4/500\n",
      " - 0s - loss: 3.2524 - acc: 0.0400\n",
      "Epoch 5/500\n",
      " - 0s - loss: 3.2495 - acc: 0.0400\n",
      "Epoch 6/500\n",
      " - 0s - loss: 3.2470 - acc: 0.0400\n",
      "Epoch 7/500\n",
      " - 0s - loss: 3.2440 - acc: 0.0400\n",
      "Epoch 8/500\n",
      " - 0s - loss: 3.2411 - acc: 0.0400\n",
      "Epoch 9/500\n",
      " - 0s - loss: 3.2378 - acc: 0.0400\n",
      "Epoch 10/500\n",
      " - 0s - loss: 3.2348 - acc: 0.0400\n",
      "...\n"
     ]
    },    
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [      
      "Epoch 490/500\n",
      " - 0s - loss: 1.7054 - acc: 0.7200\n",
      "Epoch 491/500\n",
      " - 0s - loss: 1.7041 - acc: 0.7600\n",
      "Epoch 492/500\n",
      " - 0s - loss: 1.7029 - acc: 0.8800\n",
      "Epoch 493/500\n",
      " - 0s - loss: 1.7021 - acc: 0.7600\n",
      "Epoch 494/500\n",
      " - 0s - loss: 1.7024 - acc: 0.8800\n",
      "Epoch 495/500\n",
      " - 0s - loss: 1.6992 - acc: 0.7600\n",
      "Epoch 496/500\n",
      " - 0s - loss: 1.7001 - acc: 0.8000\n",
      "Epoch 497/500\n",
      " - 0s - loss: 1.6995 - acc: 0.6800\n",
      "Epoch 498/500\n",
      " - 0s - loss: 1.6994 - acc: 0.7600\n",
      "Epoch 499/500\n",
      " - 0s - loss: 1.7001 - acc: 0.8000\n",
      "Epoch 500/500\n",
      " - 0s - loss: 1.6963 - acc: 0.8400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2145c550>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP7. 評估模型準確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 92.00%\n"
     ]
    }
   ],
   "source": [
    "# 評估模型的性能\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP8. 預測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A'] -> B\n",
      "['B'] -> C\n",
      "['C'] -> D\n",
      "['D'] -> E\n",
      "['E'] -> F\n",
      "['F'] -> G\n",
      "['G'] -> H\n",
      "['H'] -> I\n",
      "['I'] -> J\n",
      "['J'] -> K\n",
      "['K'] -> L\n",
      "['L'] -> M\n",
      "['M'] -> N\n",
      "['N'] -> O\n",
      "['O'] -> P\n",
      "['P'] -> Q\n",
      "['Q'] -> R\n",
      "['R'] -> S\n",
      "['S'] -> T\n",
      "['T'] -> U\n",
      "['U'] -> V\n",
      "['V'] -> W\n",
      "['W'] -> Y\n",
      "['X'] -> Z\n",
      "['Y'] -> Z\n"
     ]
    }
   ],
   "source": [
    "# 展示模型預測能力\n",
    "for pattern in dataX:\n",
    "    # 把26個字母一個個拿進模型來預測會出現的字母\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    \n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction) # 機率最大的idx\n",
    "    result = int_to_char[index] # 看看預測出來的是那一個字母\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print(seq_in, \"->\", result) # 打印結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們可以看到，\"序列資料的預測\"這個問題對於網絡學習確實是困難的。\n",
    "原因是，在以上的範例中的LSTM單位沒有任何上下文的知識(時間歩長只有\"1\")。每個輸入輸出模式以隨機順序(shuffle)出現到人工網網絡上，而且Keras的LSTM網絡內步狀態(state)會在每個訓練循環(epoch)後被重置(reset)。\n",
    "\n",
    "接下來，讓我們嘗試提供更多的順序資訊來讓LSTM學習。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型 2. LSTM 學習三個字符特徵窗口(Three-Char Feature Window)到一個字符映射\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP1. 準備訓練用資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC -> D\n",
      "BCD -> E\n",
      "CDE -> F\n",
      "DEF -> G\n",
      "EFG -> H\n",
      "FGH -> I\n",
      "GHI -> J\n",
      "HIJ -> K\n",
      "IJK -> L\n",
      "JKL -> M\n",
      "KLM -> N\n",
      "LMN -> O\n",
      "MNO -> P\n",
      "NOP -> Q\n",
      "OPQ -> R\n",
      "PQR -> S\n",
      "QRS -> T\n",
      "RST -> U\n",
      "STU -> V\n",
      "TUV -> W\n",
      "UVW -> X\n",
      "VWX -> Y\n",
      "WXY -> Z\n"
     ]
    }
   ],
   "source": [
    "# 準備輸入數據集\n",
    "seq_length = 3 # 這次我們要準備3個時間步長的資料\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "    seq_in = alphabet[i:i + seq_length] # 3個字符\n",
    "    seq_out = alphabet[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    print(seq_in, '->', seq_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP2. 資料預處理\n",
    "\n",
    "\n",
    "> ABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "\n",
    "> 例如: \n",
    "\n",
    "> 給 HIJ -> 預測 K\n",
    "\n",
    "> 給 EFG -> 預測 H\n",
    "\n",
    "目標訓練張量結構: (samples, time_steps, features) -> (n , **1**, **3** )\n",
    "\n",
    "請特別注意, 這裡的三個字符會變成一個有3個element的\"feature\" vector。因此在準備訓練資料集的時候, 1筆訓練資料只有\"1\"個時間步, 裡頭存放著\"3\"個字符的資料\"features\"向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (23, 1, 3)\n",
      "y shape:  (23, 26)\n"
     ]
    }
   ],
   "source": [
    "# 重塑 X 資料的維度成為 (samples, time_steps, features)\n",
    "X = numpy.reshape(dataX, (len(dataX), 1, seq_length))  # <-- 特別注意這裡\n",
    "\n",
    "# 歸一化\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# 使用one hot encode 對Y值進行編碼\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"y shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP3. 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 32)                4608      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 26)                858       \n",
      "=================================================================\n",
      "Total params: 5,466\n",
      "Trainable params: 5,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 創建模型\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2]))) # <-- 特別注意這裡\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP4. 定義訓練並進行訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " - 2s - loss: 3.2753 - acc: 0.0435\n",
      "Epoch 2/500\n",
      " - 0s - loss: 3.2629 - acc: 0.0000e+00\n",
      "Epoch 3/500\n",
      " - 0s - loss: 3.2556 - acc: 0.0000e+00\n",
      "Epoch 4/500\n",
      " - 0s - loss: 3.2487 - acc: 0.0435\n",
      "Epoch 5/500\n",
      " - 0s - loss: 3.2420 - acc: 0.0435\n",
      "Epoch 6/500\n",
      " - 0s - loss: 3.2355 - acc: 0.0435\n",
      "Epoch 7/500\n",
      " - 0s - loss: 3.2294 - acc: 0.0435\n",
      "Epoch 8/500\n",
      " - 0s - loss: 3.2214 - acc: 0.0435\n",
      "Epoch 9/500\n",
      " - 0s - loss: 3.2142 - acc: 0.0435\n",
      "Epoch 10/500\n",
      " - 0s - loss: 3.2056 - acc: 0.0435\n",
      "...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [      
      "Epoch 490/500\n",
      " - 0s - loss: 1.6114 - acc: 0.7826\n",
      "Epoch 491/500\n",
      " - 0s - loss: 1.6089 - acc: 0.7826\n",
      "Epoch 492/500\n",
      " - 0s - loss: 1.6108 - acc: 0.8261\n",
      "Epoch 493/500\n",
      " - 0s - loss: 1.6091 - acc: 0.7826\n",
      "Epoch 494/500\n",
      " - 0s - loss: 1.6057 - acc: 0.7826\n",
      "Epoch 495/500\n",
      " - 0s - loss: 1.6060 - acc: 0.7826\n",
      "Epoch 496/500\n",
      " - 0s - loss: 1.6058 - acc: 0.8261\n",
      "Epoch 497/500\n",
      " - 0s - loss: 1.6045 - acc: 0.7826\n",
      "Epoch 498/500\n",
      " - 0s - loss: 1.6042 - acc: 0.7826\n",
      "Epoch 499/500\n",
      " - 0s - loss: 1.6006 - acc: 0.8696\n",
      "Epoch 500/500\n",
      " - 0s - loss: 1.6011 - acc: 0.7826\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x235b8d68>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP5. 評估模型準確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 82.61%\n"
     ]
    }
   ],
   "source": [
    "# 評估模型的性能\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP6. 預測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C'] -> D\n",
      "['B', 'C', 'D'] -> E\n",
      "['C', 'D', 'E'] -> F\n",
      "['D', 'E', 'F'] -> G\n",
      "['E', 'F', 'G'] -> H\n",
      "['F', 'G', 'H'] -> I\n",
      "['G', 'H', 'I'] -> J\n",
      "['H', 'I', 'J'] -> K\n",
      "['I', 'J', 'K'] -> L\n",
      "['J', 'K', 'L'] -> M\n",
      "['K', 'L', 'M'] -> N\n",
      "['L', 'M', 'N'] -> O\n",
      "['M', 'N', 'O'] -> P\n",
      "['N', 'O', 'P'] -> Q\n",
      "['O', 'P', 'Q'] -> R\n",
      "['P', 'Q', 'R'] -> S\n",
      "['Q', 'R', 'S'] -> T\n",
      "['R', 'S', 'T'] -> U\n",
      "['S', 'T', 'U'] -> W\n",
      "['T', 'U', 'V'] -> X\n",
      "['U', 'V', 'W'] -> Z\n",
      "['V', 'W', 'X'] -> Z\n",
      "['W', 'X', 'Y'] -> Z\n"
     ]
    }
   ],
   "source": [
    "# 展示一些模型預測\n",
    "for pattern in dataX:\n",
    "    x = numpy.reshape(pattern, (1, 1, len(pattern)))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們可以看到，\"模型#2\"相比於\"模型#1\"在預測的表現上只有小幅提升。這個簡單的問題，即使使用window方法，我們仍然無法讓LSTM學習到預測正確的字母出現的順序。\n",
    "\n",
    "以上的範例也是一個誤用LSTM網絡的糟糕的張量結構。事實上，字母序列是一個特徵的\"時間步驟(timesteps)\"，而不是單獨特徵的一個時間步驟。我們已經給了網絡更多的上下文，但是沒有更多的順序上下文(context)。\n",
    "\n",
    "下一範例中，我們將以\"時間步驟(timesteps)\"的形式給出更多的上下文(context)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型 3. LSTM 學習三個字符的時間步驟窗口(Three-Char Time Step Window)到一個字符的映射"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP1. 準備訓練用資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC -> D\n",
      "BCD -> E\n",
      "CDE -> F\n",
      "DEF -> G\n",
      "EFG -> H\n",
      "FGH -> I\n",
      "GHI -> J\n",
      "HIJ -> K\n",
      "IJK -> L\n",
      "JKL -> M\n",
      "KLM -> N\n",
      "LMN -> O\n",
      "MNO -> P\n",
      "NOP -> Q\n",
      "OPQ -> R\n",
      "PQR -> S\n",
      "QRS -> T\n",
      "RST -> U\n",
      "STU -> V\n",
      "TUV -> W\n",
      "UVW -> X\n",
      "VWX -> Y\n",
      "WXY -> Z\n"
     ]
    }
   ],
   "source": [
    "seq_length = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "    seq_in = alphabet[i:i + seq_length]\n",
    "    seq_out = alphabet[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    print(seq_in, '->', seq_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP2. 資料預處理\n",
    "\n",
    "\n",
    "> ABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
    "\n",
    "> 例如: \n",
    "\n",
    "> 給 HIJ -> 預測 K\n",
    "\n",
    "> 給 EFG -> 預測 H\n",
    "\n",
    "目標訓練張量結構: (samples, time_steps, features) -> (n , **3**, **1** )\n",
    "\n",
    "準備訓練資料集的時候要把資料的張量結構轉換成, 1筆訓練資料有\"3\"個時間步, 裡頭存放著\"1\"個字符的資料\"features\"向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 重塑 X 資料的維度成為 (samples, time_steps, features)\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))  # <-- 特別注意這裡\n",
    "\n",
    "# 歸一化\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# 使用one hot encode 對Y值進行編碼\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP3. 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 32)                4352      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 26)                858       \n",
      "=================================================================\n",
      "Total params: 5,210\n",
      "Trainable params: 5,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 創建模型\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2]))) # <-- 特別注意這裡\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP4. 定義訓練並進行訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " - 2s - loss: 3.2632 - acc: 0.0000e+00\n",
      "Epoch 2/500\n",
      " - 0s - loss: 3.2500 - acc: 0.0000e+00\n",
      "Epoch 3/500\n",
      " - 0s - loss: 3.2425 - acc: 0.0435\n",
      "Epoch 4/500\n",
      " - 0s - loss: 3.2357 - acc: 0.0000e+00\n",
      "Epoch 5/500\n",
      " - 0s - loss: 3.2285 - acc: 0.0000e+00\n",
      "Epoch 6/500\n",
      " - 0s - loss: 3.2207 - acc: 0.0435\n",
      "Epoch 7/500\n",
      " - 0s - loss: 3.2125 - acc: 0.0435\n",
      "Epoch 8/500\n",
      " - 0s - loss: 3.2036 - acc: 0.0435\n",
      "Epoch 9/500\n",
      " - 0s - loss: 3.1919 - acc: 0.0435\n",
      "Epoch 10/500\n",
      " - 0s - loss: 3.1821 - acc: 0.0435\n",
      "...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [      
      "Epoch 490/500\n",
      " - 0s - loss: 0.2457 - acc: 1.0000\n",
      "Epoch 491/500\n",
      " - 0s - loss: 0.2387 - acc: 1.0000\n",
      "Epoch 492/500\n",
      " - 0s - loss: 0.2394 - acc: 1.0000\n",
      "Epoch 493/500\n",
      " - 0s - loss: 0.2384 - acc: 1.0000\n",
      "Epoch 494/500\n",
      " - 0s - loss: 0.2416 - acc: 1.0000\n",
      "Epoch 495/500\n",
      " - 0s - loss: 0.2385 - acc: 1.0000\n",
      "Epoch 496/500\n",
      " - 0s - loss: 0.2380 - acc: 1.0000\n",
      "Epoch 497/500\n",
      " - 0s - loss: 0.2331 - acc: 1.0000\n",
      "Epoch 498/500\n",
      " - 0s - loss: 0.2341 - acc: 1.0000\n",
      "Epoch 499/500\n",
      " - 0s - loss: 0.2371 - acc: 1.0000\n",
      "Epoch 500/500\n",
      " - 0s - loss: 0.2325 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x261e7ac8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP5. 評估模型準確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# 評估模型的性能\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP6. 預測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B', 'C'] -> D\n",
      "['B', 'C', 'D'] -> E\n",
      "['C', 'D', 'E'] -> F\n",
      "['D', 'E', 'F'] -> G\n",
      "['E', 'F', 'G'] -> H\n",
      "['F', 'G', 'H'] -> I\n",
      "['G', 'H', 'I'] -> J\n",
      "['H', 'I', 'J'] -> K\n",
      "['I', 'J', 'K'] -> L\n",
      "['J', 'K', 'L'] -> M\n",
      "['K', 'L', 'M'] -> N\n",
      "['L', 'M', 'N'] -> O\n",
      "['M', 'N', 'O'] -> P\n",
      "['N', 'O', 'P'] -> Q\n",
      "['O', 'P', 'Q'] -> R\n",
      "['P', 'Q', 'R'] -> S\n",
      "['Q', 'R', 'S'] -> T\n",
      "['R', 'S', 'T'] -> U\n",
      "['S', 'T', 'U'] -> V\n",
      "['T', 'U', 'V'] -> W\n",
      "['U', 'V', 'W'] -> X\n",
      "['V', 'W', 'X'] -> Y\n",
      "['W', 'X', 'Y'] -> Z\n"
     ]
    }
   ],
   "source": [
    "# 讓我們擷取3個字符轉成張量結構 shape:(1,3,1)來進行infer\n",
    "for pattern in dataX:\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由\"模型#3\"的表現來看, 當我們以\"時間步驟(timesteps)\"的形式給出更多的上下文(context)來訓練LSTM模型時, 這時候循環神經網絡在序列資料的學習的效果就可以發揮出它的效用。\n",
    "\n",
    "\"模型#3\"在驗證的結果可達到100%的預測準確度(在這個很簡單的26個字母的順序預測的任務上)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型 4. LSTM學習可變長度字符輸入到單字符輸出\n",
    "\n",
    "讓我們建立一個模型，來接受\"變動字母序列(variable-length)\"的輸入來預測下一個字母。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP1. 準備訓練用資料\n",
    "\n",
    "為了簡化，我們將定義一個最大輸入序列長度(比如說\"5\", 代表輸入的序列可以是 1 ~ 5)，以加速訓練。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UVWXY -> Z\n",
      "UVWXY -> Z\n",
      "EFGH -> I\n",
      "GHIJ -> K\n",
      "EFGH -> I\n",
      "DEF -> G\n",
      "CDEFG -> H\n",
      "OP -> Q\n",
      "X -> Y\n",
      "TU -> V\n",
      "IJK -> L\n",
      "LMNOP -> Q\n",
      "T -> U\n",
      "UVWX -> Y\n",
      "X -> Y\n",
      "H -> I\n",
      "EFGHI -> J\n",
      "QRSTU -> V\n",
      "EFG -> H\n",
      "RSTU -> V\n",
      "QRST -> U\n",      
      "QR -> S\n",
      "JK -> L\n",
      "GHI -> J\n",
      "KL -> M\n",
      "BCDE -> F\n",
      "AB -> C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KLMNO -> P\n",
      "UVWXY -> Z\n",
      "EFGH -> I\n",
      "FG -> H\n",
      "DEF -> G\n",
      "STU -> V\n",
      "FGHI -> J\n",
      "OP -> Q\n",
      "FGHIJ -> K\n",
      "LMNOP -> Q\n",
      "DEF -> G\n",
      "W -> X\n",
      "KLMN -> O\n",
      "WXY -> Z\n",
      "PQRST -> U\n",
      "LMNOP -> Q\n",
      "PQ -> R\n",
      "FGHI -> J\n",
      "QRS -> T\n",
      "CDEFG -> H\n",
      "VW -> X\n",
      "DEF -> G\n",
	  "..."
     ]
    }
   ],
   "source": [
    "# 準備訓練資料\n",
    "num_inputs = 1000\n",
    "max_len = 5 # 最大序列長度\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(num_inputs):\n",
    "    start = numpy.random.randint(len(alphabet)-2)\n",
    "    end = numpy.random.randint(start, min(start+max_len,len(alphabet)-1))\n",
    "    sequence_in = alphabet[start:end+1]\n",
    "    sequence_out = alphabet[end + 1]\n",
    "    dataX.append([char_to_int[char] for char in sequence_in])\n",
    "    dataY.append(char_to_int[sequence_out])\n",
    "    print(sequence_in, '->', sequence_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP2. 資料預處理\n",
    "因為輸入序列的長度會在1到max_len之間變動，因此需要以\"0\"來填充(padding)。在這裡，我們使用Keras內附的pad_sequences（）函數並設定使用左側（前綴）填充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 將訓練資料轉換為陣列和並進行序列填充（如果需要）\n",
    "X = pad_sequences(dataX, maxlen=max_len, dtype='float32') # <-- 注意這裡\n",
    "\n",
    "# 重塑 X 資料的維度成為 (samples, time_steps, features)\n",
    "X = numpy.reshape(X, (X.shape[0], max_len, 1)) # <-- 特別注意這裡\n",
    "\n",
    "# 歸一化\n",
    "X = X / float(len(alphabet))\n",
    "\n",
    "# 使用one hot encode 對Y值進行編碼\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP3. 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 32)                4352      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 26)                858       \n",
      "=================================================================\n",
      "Total params: 5,210\n",
      "Trainable params: 5,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 創建模型\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], 1))) # <-- 注意這裡\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP4. 定義訓練並進行訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " - 7s - loss: 3.0984 - acc: 0.0770\n",
      "Epoch 2/500\n",
      " - 6s - loss: 2.8499 - acc: 0.1180\n",
      "Epoch 3/500\n",
      " - 7s - loss: 2.5159 - acc: 0.1990\n",
      "Epoch 4/500\n",
      " - 6s - loss: 2.2225 - acc: 0.2440\n",
      "Epoch 5/500\n",
      " - 6s - loss: 2.0490 - acc: 0.2880\n",
      "Epoch 6/500\n",
      " - 5s - loss: 1.9284 - acc: 0.3050\n",
      "Epoch 7/500\n",
      " - 5s - loss: 1.8142 - acc: 0.3530\n",
      "Epoch 8/500\n",
      " - 6s - loss: 1.7206 - acc: 0.3940\n",
      "Epoch 9/500\n",
      " - 6s - loss: 1.6397 - acc: 0.4070\n",
      "Epoch 10/500\n",
      " - 6s - loss: 1.5607 - acc: 0.4340\n",
      "Epoch 11/500\n",
      " - 6s - loss: 1.4909 - acc: 0.4880\n",
      "Epoch 12/500\n",
      " - 5s - loss: 1.4180 - acc: 0.5140\n",
      "Epoch 13/500\n",
      " - 6s - loss: 1.3613 - acc: 0.5570\n",
      "Epoch 14/500\n",
      " - 6s - loss: 1.3055 - acc: 0.5570\n",
      "Epoch 15/500\n",
      " - 6s - loss: 1.2513 - acc: 0.6100\n",
      "Epoch 16/500\n",
      " - 6s - loss: 1.2058 - acc: 0.6080\n",
      "Epoch 17/500\n",
      " - 6s - loss: 1.1530 - acc: 0.6150\n",
      "Epoch 18/500\n",
      " - 6s - loss: 1.1126 - acc: 0.6290\n",
      "Epoch 19/500\n",
      " - 6s - loss: 1.0732 - acc: 0.6700\n",
      "Epoch 20/500\n",
      " - 6s - loss: 1.0427 - acc: 0.6540\n",
      "Epoch 21/500\n",
      " - 6s - loss: 1.0019 - acc: 0.6760\n",
      "Epoch 22/500\n",
      " - 6s - loss: 0.9780 - acc: 0.6870\n",
      "Epoch 23/500\n",
      " - 6s - loss: 0.9380 - acc: 0.7330\n",
      "Epoch 24/500\n",
      " - 7s - loss: 0.9178 - acc: 0.7130\n",
      "Epoch 25/500\n",
      " - 6s - loss: 0.8939 - acc: 0.7180\n",
      "Epoch 26/500\n",
      " - 6s - loss: 0.8939 - acc: 0.7070\n",
      "Epoch 27/500\n",
      " - 6s - loss: 0.8633 - acc: 0.7270\n",
      "Epoch 28/500\n",
      " - 6s - loss: 0.8175 - acc: 0.7530\n",
      "Epoch 29/500\n",
      " - 6s - loss: 0.8099 - acc: 0.7480\n",
      "Epoch 30/500\n",
      " - 6s - loss: 0.8044 - acc: 0.7310\n",
      "Epoch 31/500\n",
      " - 7s - loss: 0.7683 - acc: 0.7620\n",
      "Epoch 32/500\n",
      " - 6s - loss: 0.7619 - acc: 0.7480\n",
      "Epoch 33/500\n",
      " - 6s - loss: 0.7473 - acc: 0.7680\n",
      "Epoch 34/500\n",
      " - 6s - loss: 0.7229 - acc: 0.7800\n",
      "Epoch 35/500\n",
      " - 6s - loss: 0.7192 - acc: 0.7770\n",
      "Epoch 36/500\n",
      " - 6s - loss: 0.7172 - acc: 0.7670\n",
      "Epoch 37/500\n",
      " - 6s - loss: 0.7025 - acc: 0.7810\n",
      "Epoch 38/500\n",
      " - 6s - loss: 0.6931 - acc: 0.7850\n",
      "Epoch 39/500\n",
      " - 6s - loss: 0.6596 - acc: 0.8030\n",
      "Epoch 40/500\n",
      " - 6s - loss: 0.6812 - acc: 0.7720\n",
      "Epoch 41/500\n",
      " - 5s - loss: 0.6458 - acc: 0.7970\n",
      "Epoch 42/500\n",
      " - 5s - loss: 0.6404 - acc: 0.7980\n",
      "Epoch 43/500\n",
      " - 6s - loss: 0.6408 - acc: 0.7920\n",
      "Epoch 44/500\n",
      " - 5s - loss: 0.6410 - acc: 0.7840\n",
      "Epoch 45/500\n",
      " - 6s - loss: 0.6374 - acc: 0.7940\n",
      "Epoch 46/500\n",
      " - 5s - loss: 0.6144 - acc: 0.8050\n",
      "Epoch 47/500\n",
      " - 5s - loss: 0.5986 - acc: 0.8160\n",
      "Epoch 48/500\n",
      " - 6s - loss: 0.6063 - acc: 0.7890\n",
      "Epoch 49/500\n",
      " - 5s - loss: 0.5805 - acc: 0.8230\n",
      "Epoch 50/500\n",
      " - 6s - loss: 0.5959 - acc: 0.8120\n",
      "Epoch 51/500\n",
      " - 6s - loss: 0.5672 - acc: 0.8220\n",
      "Epoch 52/500\n",
      " - 5s - loss: 0.5843 - acc: 0.8140\n",
      "Epoch 53/500\n",
      " - 5s - loss: 0.5486 - acc: 0.8390\n",
      "Epoch 54/500\n",
      " - 6s - loss: 0.5593 - acc: 0.8190\n",
      "Epoch 55/500\n",
      " - 6s - loss: 0.6082 - acc: 0.7890\n",
      "Epoch 56/500\n",
      " - 6s - loss: 0.5170 - acc: 0.8450\n",
      "Epoch 57/500\n",
      " - 6s - loss: 0.5791 - acc: 0.8060\n",
      "Epoch 58/500\n",
      " - 6s - loss: 0.5185 - acc: 0.8510\n",
      "Epoch 59/500\n",
      " - 6s - loss: 0.5380 - acc: 0.8220\n",
      "Epoch 60/500\n",
      " - 7s - loss: 0.5129 - acc: 0.8340\n",
      "Epoch 61/500\n",
      " - 6s - loss: 0.5086 - acc: 0.8440\n",
      "Epoch 62/500\n",
      " - 6s - loss: 0.5134 - acc: 0.8340\n",
      "Epoch 63/500\n",
      " - 6s - loss: 0.5156 - acc: 0.8320\n",
      "Epoch 64/500\n",
      " - 6s - loss: 0.5282 - acc: 0.8280\n",
      "Epoch 65/500\n",
      " - 5s - loss: 0.5156 - acc: 0.8420\n",
      "Epoch 66/500\n",
      " - 5s - loss: 0.4822 - acc: 0.8520\n",
      "Epoch 67/500\n",
      " - 6s - loss: 0.4788 - acc: 0.8560\n",
      "Epoch 68/500\n",
      " - 5s - loss: 0.5378 - acc: 0.8150\n",
      "Epoch 69/500\n",
      " - 5s - loss: 0.4579 - acc: 0.8620\n",
      "Epoch 70/500\n",
      " - 6s - loss: 0.4649 - acc: 0.8570\n",
      "Epoch 71/500\n",
      " - 6s - loss: 0.4579 - acc: 0.8490\n",
      "Epoch 72/500\n",
      " - 6s - loss: 0.4668 - acc: 0.8330\n",
      "Epoch 73/500\n",
      " - 6s - loss: 0.5284 - acc: 0.8180\n",
      "Epoch 74/500\n",
      " - 5s - loss: 0.4332 - acc: 0.8730\n",
      "Epoch 75/500\n",
      " - 6s - loss: 0.4538 - acc: 0.8560\n",
      "Epoch 76/500\n",
      " - 6s - loss: 0.4661 - acc: 0.8470\n",
      "Epoch 77/500\n",
      " - 5s - loss: 0.4271 - acc: 0.8710\n",
      "Epoch 78/500\n",
      " - 6s - loss: 0.4410 - acc: 0.8570\n",
      "Epoch 79/500\n",
      " - 6s - loss: 0.4628 - acc: 0.8570\n",
      "Epoch 80/500\n",
      " - 6s - loss: 0.4379 - acc: 0.8620\n",
      "Epoch 81/500\n",
      " - 6s - loss: 0.4338 - acc: 0.8550\n",
      "Epoch 82/500\n",
      " - 6s - loss: 0.5099 - acc: 0.8430\n",
      "Epoch 83/500\n",
      " - 6s - loss: 0.4032 - acc: 0.8780\n",
      "Epoch 84/500\n",
      " - 6s - loss: 0.4203 - acc: 0.8570\n",
      "Epoch 85/500\n",
      " - 6s - loss: 0.4267 - acc: 0.8600\n",
      "Epoch 86/500\n",
      " - 6s - loss: 0.3949 - acc: 0.8780\n",
      "Epoch 87/500\n",
      " - 6s - loss: 0.4777 - acc: 0.8470\n",
      "Epoch 88/500\n",
      " - 6s - loss: 0.3985 - acc: 0.8790\n",
      "Epoch 89/500\n",
      " - 6s - loss: 0.4102 - acc: 0.8630\n",
      "Epoch 90/500\n",
      " - 6s - loss: 0.4162 - acc: 0.8640\n",
      "Epoch 91/500\n",
      " - 6s - loss: 0.3855 - acc: 0.8740\n",
      "Epoch 92/500\n",
      " - 6s - loss: 0.4495 - acc: 0.8520\n",
      "Epoch 93/500\n",
      " - 6s - loss: 0.3803 - acc: 0.8830\n",
      "Epoch 94/500\n",
      " - 7s - loss: 0.3796 - acc: 0.8820\n",
      "Epoch 95/500\n",
      " - 5s - loss: 0.5199 - acc: 0.8280\n",
      "Epoch 96/500\n",
      " - 5s - loss: 0.3668 - acc: 0.8880\n",
      "Epoch 97/500\n",
      " - 6s - loss: 0.3731 - acc: 0.8810\n",
      "Epoch 98/500\n",
      " - 6s - loss: 0.4230 - acc: 0.8520\n",
      "Epoch 99/500\n",
      " - 5s - loss: 0.3644 - acc: 0.8760\n",
      "Epoch 100/500\n",
      " - 6s - loss: 0.3860 - acc: 0.8750\n",
      "Epoch 101/500\n",
      " - 6s - loss: 0.4050 - acc: 0.8680\n",
      "Epoch 102/500\n",
      " - 5s - loss: 0.3946 - acc: 0.8810\n",
      "Epoch 103/500\n",
      " - 5s - loss: 0.3590 - acc: 0.8790\n",
      "Epoch 104/500\n",
      " - 5s - loss: 0.3592 - acc: 0.8860\n",
      "Epoch 105/500\n",
      " - 6s - loss: 0.3708 - acc: 0.8840\n",
      "Epoch 106/500\n",
      " - 6s - loss: 0.4420 - acc: 0.8540\n",
      "Epoch 107/500\n",
      " - 6s - loss: 0.3432 - acc: 0.8970\n",
      "Epoch 108/500\n",
      " - 6s - loss: 0.3443 - acc: 0.8880\n",
      "Epoch 109/500\n",
      " - 6s - loss: 0.3610 - acc: 0.8810\n",
      "Epoch 110/500\n",
      " - 6s - loss: 0.4282 - acc: 0.8530\n",
      "Epoch 111/500\n",
      " - 6s - loss: 0.3425 - acc: 0.9000\n",
      "Epoch 112/500\n",
      " - 6s - loss: 0.3441 - acc: 0.8860\n",
      "Epoch 113/500\n",
      " - 6s - loss: 0.3912 - acc: 0.8830\n",
      "Epoch 114/500\n",
      " - 6s - loss: 0.3521 - acc: 0.8890\n",
      "Epoch 115/500\n",
      " - 5s - loss: 0.3361 - acc: 0.8910\n",
      "Epoch 116/500\n",
      " - 6s - loss: 0.3507 - acc: 0.8870\n",
      "Epoch 117/500\n",
      " - 6s - loss: 0.3391 - acc: 0.8850\n",
      "Epoch 118/500\n",
      " - 6s - loss: 0.3701 - acc: 0.8830\n",
      "Epoch 119/500\n",
      " - 6s - loss: 0.3234 - acc: 0.9020\n",
      "Epoch 120/500\n",
      " - 6s - loss: 0.3307 - acc: 0.8850\n",
      "Epoch 121/500\n",
      " - 6s - loss: 0.3991 - acc: 0.8670\n",
      "Epoch 122/500\n",
      " - 6s - loss: 0.3140 - acc: 0.9100\n",
      "Epoch 123/500\n",
      " - 5s - loss: 0.3366 - acc: 0.8890\n",
      "Epoch 124/500\n",
      " - 6s - loss: 0.3207 - acc: 0.8950\n",
      "Epoch 125/500\n",
      " - 6s - loss: 0.3164 - acc: 0.9010\n",
      "Epoch 126/500\n",
      " - 6s - loss: 0.3709 - acc: 0.8670\n",
      "Epoch 127/500\n",
      " - 6s - loss: 0.3109 - acc: 0.9040\n",
      "Epoch 128/500\n",
      " - 5s - loss: 0.4421 - acc: 0.8530\n",
      "Epoch 129/500\n",
      " - 6s - loss: 0.3061 - acc: 0.9080\n",
      "Epoch 130/500\n",
      " - 6s - loss: 0.3035 - acc: 0.9020\n",
      "Epoch 131/500\n",
      " - 6s - loss: 0.3124 - acc: 0.8960\n",
      "Epoch 132/500\n",
      " - 6s - loss: 0.3122 - acc: 0.9080\n",
      "Epoch 133/500\n",
      " - 6s - loss: 0.4044 - acc: 0.8670\n",
      "Epoch 134/500\n",
      " - 6s - loss: 0.3029 - acc: 0.9100\n",
      "Epoch 135/500\n",
      " - 6s - loss: 0.2982 - acc: 0.9100\n",
      "Epoch 136/500\n",
      " - 6s - loss: 0.3305 - acc: 0.8960\n",
      "Epoch 137/500\n",
      " - 6s - loss: 0.3042 - acc: 0.9040\n",
      "Epoch 138/500\n",
      " - 6s - loss: 0.4237 - acc: 0.8700\n",
      "Epoch 139/500\n",
      " - 5s - loss: 0.3095 - acc: 0.9030\n",
      "Epoch 140/500\n",
      " - 5s - loss: 0.2920 - acc: 0.9190\n",
      "Epoch 141/500\n",
      " - 6s - loss: 0.2966 - acc: 0.9040\n",
      "Epoch 142/500\n",
      " - 6s - loss: 0.2927 - acc: 0.9060\n",
      "Epoch 143/500\n",
      " - 5s - loss: 0.2975 - acc: 0.9040\n",
      "Epoch 144/500\n",
      " - 5s - loss: 0.4023 - acc: 0.8710\n",
      "Epoch 145/500\n",
      " - 6s - loss: 0.2824 - acc: 0.9100\n",
      "Epoch 146/500\n",
      " - 6s - loss: 0.2828 - acc: 0.9110\n",
      "Epoch 147/500\n",
      " - 6s - loss: 0.3364 - acc: 0.8970\n",
      "Epoch 148/500\n",
      " - 6s - loss: 0.3161 - acc: 0.8920\n",
      "Epoch 149/500\n",
      " - 5s - loss: 0.2774 - acc: 0.9130\n",
      "Epoch 150/500\n",
      " - 6s - loss: 0.3743 - acc: 0.8870\n",
      "Epoch 151/500\n",
      " - 6s - loss: 0.2727 - acc: 0.9120\n",
      "Epoch 152/500\n",
      " - 6s - loss: 0.2825 - acc: 0.9090\n",
      "Epoch 153/500\n",
      " - 6s - loss: 0.3575 - acc: 0.8840\n",
      "Epoch 154/500\n",
      " - 6s - loss: 0.3132 - acc: 0.9090\n",
      "Epoch 155/500\n",
      " - 6s - loss: 0.2980 - acc: 0.9120\n",
      "Epoch 156/500\n",
      " - 6s - loss: 0.2776 - acc: 0.9170\n",
      "Epoch 157/500\n",
      " - 6s - loss: 0.2756 - acc: 0.9100\n",
      "Epoch 158/500\n",
      " - 5s - loss: 0.3218 - acc: 0.8920\n",
      "Epoch 159/500\n",
      " - 6s - loss: 0.2670 - acc: 0.9150\n",
      "Epoch 160/500\n",
      " - 6s - loss: 0.3033 - acc: 0.8910\n",
      "Epoch 161/500\n",
      " - 6s - loss: 0.2705 - acc: 0.9160\n",
      "Epoch 162/500\n",
      " - 6s - loss: 0.2724 - acc: 0.9060\n",
      "Epoch 163/500\n",
      " - 6s - loss: 0.3542 - acc: 0.8840\n",
      "Epoch 164/500\n",
      " - 6s - loss: 0.2614 - acc: 0.9350\n",
      "Epoch 165/500\n",
      " - 6s - loss: 0.2643 - acc: 0.9160\n",
      "Epoch 166/500\n",
      " - 6s - loss: 0.2659 - acc: 0.9120\n",
      "Epoch 167/500\n",
      " - 6s - loss: 0.2736 - acc: 0.9130\n",
      "Epoch 168/500\n",
      " - 5s - loss: 0.2610 - acc: 0.9140\n",
      "Epoch 169/500\n",
      " - 5s - loss: 0.2624 - acc: 0.9130\n",
      "Epoch 170/500\n",
      " - 5s - loss: 0.3537 - acc: 0.8990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 171/500\n",
      " - 6s - loss: 0.2559 - acc: 0.9190\n",
      "Epoch 172/500\n",
      " - 6s - loss: 0.2532 - acc: 0.9160\n",
      "Epoch 173/500\n",
      " - 6s - loss: 0.2594 - acc: 0.9160\n",
      "Epoch 174/500\n",
      " - 6s - loss: 0.2829 - acc: 0.9110\n",
      "Epoch 175/500\n",
      " - 6s - loss: 0.3482 - acc: 0.9040\n",
      "Epoch 176/500\n",
      " - 6s - loss: 0.2502 - acc: 0.9180\n",
      "Epoch 177/500\n",
      " - 6s - loss: 0.2543 - acc: 0.9200\n",
      "Epoch 178/500\n",
      " - 6s - loss: 0.2520 - acc: 0.9210\n",
      "Epoch 179/500\n",
      " - 6s - loss: 0.2763 - acc: 0.9080\n",
      "Epoch 180/500\n",
      " - 6s - loss: 0.2489 - acc: 0.9210\n",
      "Epoch 181/500\n",
      " - 6s - loss: 0.2510 - acc: 0.9180\n",
      "Epoch 182/500\n",
      " - 5s - loss: 0.3519 - acc: 0.8980\n",
      "Epoch 183/500\n",
      " - 6s - loss: 0.2408 - acc: 0.9260\n",
      "Epoch 184/500\n",
      " - 6s - loss: 0.2476 - acc: 0.9230\n",
      "Epoch 185/500\n",
      " - 6s - loss: 0.3063 - acc: 0.9120\n",
      "Epoch 186/500\n",
      " - 6s - loss: 0.2374 - acc: 0.9310\n",
      "Epoch 187/500\n",
      " - 5s - loss: 0.2415 - acc: 0.9260\n",
      "Epoch 188/500\n",
      " - 6s - loss: 0.2417 - acc: 0.9210\n",
      "Epoch 189/500\n",
      " - 5s - loss: 0.3521 - acc: 0.8820\n",
      "Epoch 190/500\n",
      " - 5s - loss: 0.2310 - acc: 0.9420\n",
      "Epoch 191/500\n",
      " - 5s - loss: 0.2343 - acc: 0.9270\n",
      "Epoch 192/500\n",
      " - 5s - loss: 0.2391 - acc: 0.9150\n",
      "Epoch 193/500\n",
      " - 5s - loss: 0.3496 - acc: 0.9070\n",
      "Epoch 194/500\n",
      " - 6s - loss: 0.2269 - acc: 0.9340\n",
      "Epoch 195/500\n",
      " - 5s - loss: 0.2343 - acc: 0.9270\n",
      "Epoch 196/500\n",
      " - 5s - loss: 0.3212 - acc: 0.8980\n",
      "Epoch 197/500\n",
      " - 5s - loss: 0.2424 - acc: 0.9300\n",
      "Epoch 198/500\n",
      " - 6s - loss: 0.2290 - acc: 0.9330\n",
      "Epoch 199/500\n",
      " - 5s - loss: 0.2299 - acc: 0.9290\n",
      "Epoch 200/500\n",
      " - 6s - loss: 0.2342 - acc: 0.9260\n",
      "Epoch 201/500\n",
      " - 5s - loss: 0.2373 - acc: 0.9220\n",
      "Epoch 202/500\n",
      " - 7s - loss: 0.2308 - acc: 0.9290\n",
      "Epoch 203/500\n",
      " - 7s - loss: 0.2960 - acc: 0.9020\n",
      "Epoch 204/500\n",
      " - 8s - loss: 0.2223 - acc: 0.9410\n",
      "Epoch 205/500\n",
      " - 8s - loss: 0.2213 - acc: 0.9410\n",
      "Epoch 206/500\n",
      " - 7s - loss: 0.2244 - acc: 0.9260\n",
      "Epoch 207/500\n",
      " - 6s - loss: 0.2259 - acc: 0.9300\n",
      "Epoch 208/500\n",
      " - 7s - loss: 0.2198 - acc: 0.9380\n",
      "Epoch 209/500\n",
      " - 6s - loss: 0.2916 - acc: 0.9080\n",
      "Epoch 210/500\n",
      " - 6s - loss: 0.2169 - acc: 0.9460\n",
      "Epoch 211/500\n",
      " - 6s - loss: 0.2141 - acc: 0.9400\n",
      "Epoch 212/500\n",
      " - 7s - loss: 0.2174 - acc: 0.9330\n",
      "Epoch 213/500\n",
      " - 7s - loss: 0.2178 - acc: 0.9340\n",
      "Epoch 214/500\n",
      " - 6s - loss: 0.2167 - acc: 0.9260\n",
      "Epoch 215/500\n",
      " - 8s - loss: 0.2832 - acc: 0.9230\n",
      "Epoch 216/500\n",
      " - 6s - loss: 0.2716 - acc: 0.9230\n",
      "Epoch 217/500\n",
      " - 7s - loss: 0.2090 - acc: 0.9350\n",
      "Epoch 218/500\n",
      " - 7s - loss: 0.2145 - acc: 0.9420\n",
      "Epoch 219/500\n",
      " - 6s - loss: 0.2701 - acc: 0.9280\n",
      "Epoch 220/500\n",
      " - 6s - loss: 0.2090 - acc: 0.9350\n",
      "Epoch 221/500\n",
      " - 5s - loss: 0.2076 - acc: 0.9360\n",
      "Epoch 222/500\n",
      " - 6s - loss: 0.2115 - acc: 0.9360\n",
      "Epoch 223/500\n",
      " - 6s - loss: 0.2068 - acc: 0.9400\n",
      "Epoch 224/500\n",
      " - 5s - loss: 0.3026 - acc: 0.9190\n",
      "Epoch 225/500\n",
      " - 6s - loss: 0.2026 - acc: 0.9360\n",
      "Epoch 226/500\n",
      " - 5s - loss: 0.2058 - acc: 0.9390\n",
      "Epoch 227/500\n",
      " - 6s - loss: 0.2075 - acc: 0.9390\n",
      "Epoch 228/500\n",
      " - 6s - loss: 0.2888 - acc: 0.9090\n",
      "Epoch 229/500\n",
      " - 6s - loss: 0.2423 - acc: 0.9320\n",
      "Epoch 230/500\n",
      " - 6s - loss: 0.1951 - acc: 0.9500\n",
      "Epoch 231/500\n",
      " - 5s - loss: 0.1984 - acc: 0.9480\n",
      "Epoch 232/500\n",
      " - 6s - loss: 0.1991 - acc: 0.9430\n",
      "Epoch 233/500\n",
      " - 6s - loss: 0.2056 - acc: 0.9380\n",
      "Epoch 234/500\n",
      " - 6s - loss: 0.2049 - acc: 0.9370\n",
      "Epoch 235/500\n",
      " - 6s - loss: 0.1972 - acc: 0.9410\n",
      "Epoch 236/500\n",
      " - 6s - loss: 0.4147 - acc: 0.8940\n",
      "Epoch 237/500\n",
      " - 6s - loss: 0.1993 - acc: 0.9460\n",
      "Epoch 238/500\n",
      " - 5s - loss: 0.1899 - acc: 0.9520\n",
      "Epoch 239/500\n",
      " - 6s - loss: 0.1944 - acc: 0.9390\n",
      "Epoch 240/500\n",
      " - 6s - loss: 0.1944 - acc: 0.9410\n",
      "Epoch 241/500\n",
      " - 6s - loss: 0.1984 - acc: 0.9450\n",
      "Epoch 242/500\n",
      " - 5s - loss: 0.1968 - acc: 0.9440\n",
      "Epoch 243/500\n",
      " - 6s - loss: 0.2776 - acc: 0.9340\n",
      "Epoch 244/500\n",
      " - 6s - loss: 0.1858 - acc: 0.9500\n",
      "Epoch 245/500\n",
      " - 6s - loss: 0.1926 - acc: 0.9450\n",
      "Epoch 246/500\n",
      " - 6s - loss: 0.1913 - acc: 0.9450\n",
      "Epoch 247/500\n",
      " - 6s - loss: 0.1926 - acc: 0.9490\n",
      "Epoch 248/500\n",
      " - 7s - loss: 0.2193 - acc: 0.9350\n",
      "Epoch 249/500\n",
      " - 6s - loss: 0.1853 - acc: 0.9450\n",
      "Epoch 250/500\n",
      " - 6s - loss: 0.1874 - acc: 0.9470\n",
      "Epoch 251/500\n",
      " - 6s - loss: 0.3970 - acc: 0.8980\n",
      "Epoch 252/500\n",
      " - 6s - loss: 0.1790 - acc: 0.9640\n",
      "Epoch 253/500\n",
      " - 6s - loss: 0.1780 - acc: 0.9580\n",
      "Epoch 254/500\n",
      " - 6s - loss: 0.1802 - acc: 0.9520\n",
      "Epoch 255/500\n",
      " - 6s - loss: 0.1820 - acc: 0.9490\n",
      "Epoch 256/500\n",
      " - 6s - loss: 0.1860 - acc: 0.9400\n",
      "Epoch 257/500\n",
      " - 7s - loss: 0.2821 - acc: 0.9180\n",
      "Epoch 258/500\n",
      " - 6s - loss: 0.1963 - acc: 0.9440\n",
      "Epoch 259/500\n",
      " - 6s - loss: 0.1753 - acc: 0.9520\n",
      "Epoch 260/500\n",
      " - 5s - loss: 0.1776 - acc: 0.9640\n",
      "Epoch 261/500\n",
      " - 6s - loss: 0.1836 - acc: 0.9440\n",
      "Epoch 262/500\n",
      " - 6s - loss: 0.1828 - acc: 0.9450\n",
      "Epoch 263/500\n",
      " - 5s - loss: 0.1820 - acc: 0.9440\n",
      "Epoch 264/500\n",
      " - 7s - loss: 0.2270 - acc: 0.9300\n",
      "Epoch 265/500\n",
      " - 5s - loss: 0.1761 - acc: 0.9610\n",
      "Epoch 266/500\n",
      " - 5s - loss: 0.1731 - acc: 0.9550\n",
      "Epoch 267/500\n",
      " - 5s - loss: 0.2208 - acc: 0.9410\n",
      "Epoch 268/500\n",
      " - 5s - loss: 0.1798 - acc: 0.9590\n",
      "Epoch 269/500\n",
      " - 6s - loss: 0.1701 - acc: 0.9570\n",
      "Epoch 270/500\n",
      " - 6s - loss: 0.1741 - acc: 0.9540\n",
      "Epoch 271/500\n",
      " - 6s - loss: 0.1736 - acc: 0.9480\n",
      "Epoch 272/500\n",
      " - 6s - loss: 0.1762 - acc: 0.9470\n",
      "Epoch 273/500\n",
      " - 5s - loss: 0.2602 - acc: 0.9330\n",
      "Epoch 274/500\n",
      " - 6s - loss: 0.1880 - acc: 0.9530\n",
      "Epoch 275/500\n",
      " - 6s - loss: 0.1626 - acc: 0.9620\n",
      "Epoch 276/500\n",
      " - 6s - loss: 0.1701 - acc: 0.9540\n",
      "Epoch 277/500\n",
      " - 7s - loss: 0.1708 - acc: 0.9480\n",
      "Epoch 278/500\n",
      " - 6s - loss: 0.1752 - acc: 0.9550\n",
      "Epoch 279/500\n",
      " - 6s - loss: 0.1677 - acc: 0.9510\n",
      "Epoch 280/500\n",
      " - 6s - loss: 0.2300 - acc: 0.9310\n",
      "Epoch 281/500\n",
      " - 5s - loss: 0.2662 - acc: 0.9390\n",
      "Epoch 282/500\n",
      " - 6s - loss: 0.1565 - acc: 0.9640\n",
      "Epoch 283/500\n",
      " - 6s - loss: 0.1636 - acc: 0.9620\n",
      "Epoch 284/500\n",
      " - 6s - loss: 0.2837 - acc: 0.9310\n",
      "Epoch 285/500\n",
      " - 6s - loss: 0.1874 - acc: 0.9510\n",
      "Epoch 286/500\n",
      " - 5s - loss: 0.1634 - acc: 0.9580\n",
      "Epoch 287/500\n",
      " - 6s - loss: 0.1629 - acc: 0.9560\n",
      "Epoch 288/500\n",
      " - 6s - loss: 0.1635 - acc: 0.9570\n",
      "Epoch 289/500\n",
      " - 6s - loss: 0.1640 - acc: 0.9560\n",
      "Epoch 290/500\n",
      " - 6s - loss: 0.1662 - acc: 0.9530\n",
      "Epoch 291/500\n",
      " - 6s - loss: 0.1647 - acc: 0.9520\n",
      "Epoch 292/500\n",
      " - 6s - loss: 0.1897 - acc: 0.9480\n",
      "Epoch 293/500\n",
      " - 6s - loss: 0.1621 - acc: 0.9560\n",
      "Epoch 294/500\n",
      " - 6s - loss: 0.1594 - acc: 0.9550\n",
      "Epoch 295/500\n",
      " - 6s - loss: 0.1635 - acc: 0.9600\n",
      "Epoch 296/500\n",
      " - 5s - loss: 0.1602 - acc: 0.9560\n",
      "Epoch 297/500\n",
      " - 5s - loss: 0.1623 - acc: 0.9550\n",
      "Epoch 298/500\n",
      " - 6s - loss: 0.2056 - acc: 0.9450\n",
      "Epoch 299/500\n",
      " - 6s - loss: 0.1542 - acc: 0.9590\n",
      "Epoch 300/500\n",
      " - 6s - loss: 0.1582 - acc: 0.9600\n",
      "Epoch 301/500\n",
      " - 6s - loss: 0.1571 - acc: 0.9600\n",
      "Epoch 302/500\n",
      " - 5s - loss: 0.1616 - acc: 0.9550\n",
      "Epoch 303/500\n",
      " - 6s - loss: 0.2720 - acc: 0.9320\n",
      "Epoch 304/500\n",
      " - 6s - loss: 0.1517 - acc: 0.9570\n",
      "Epoch 305/500\n",
      " - 5s - loss: 0.1521 - acc: 0.9630\n",
      "Epoch 306/500\n",
      " - 5s - loss: 0.1532 - acc: 0.9530\n",
      "Epoch 307/500\n",
      " - 5s - loss: 0.1546 - acc: 0.9620\n",
      "Epoch 308/500\n",
      " - 5s - loss: 0.1528 - acc: 0.9600\n",
      "Epoch 309/500\n",
      " - 5s - loss: 0.1539 - acc: 0.9600\n",
      "Epoch 310/500\n",
      " - 6s - loss: 0.2544 - acc: 0.9320\n",
      "Epoch 311/500\n",
      " - 5s - loss: 0.1430 - acc: 0.9720\n",
      "Epoch 312/500\n",
      " - 5s - loss: 0.1447 - acc: 0.9690\n",
      "Epoch 313/500\n",
      " - 5s - loss: 0.1437 - acc: 0.9660\n",
      "Epoch 314/500\n",
      " - 5s - loss: 0.1507 - acc: 0.9590\n",
      "Epoch 315/500\n",
      " - 5s - loss: 0.1498 - acc: 0.9570\n",
      "Epoch 316/500\n",
      " - 5s - loss: 0.2293 - acc: 0.9400\n",
      "Epoch 317/500\n",
      " - 5s - loss: 0.1459 - acc: 0.9580\n",
      "Epoch 318/500\n",
      " - 5s - loss: 0.1456 - acc: 0.9600\n",
      "Epoch 319/500\n",
      " - 5s - loss: 0.1444 - acc: 0.9600\n",
      "Epoch 320/500\n",
      " - 5s - loss: 0.1500 - acc: 0.9520\n",
      "Epoch 321/500\n",
      " - 5s - loss: 0.1690 - acc: 0.9570\n",
      "Epoch 322/500\n",
      " - 5s - loss: 0.1926 - acc: 0.9590\n",
      "Epoch 323/500\n",
      " - 6s - loss: 0.1432 - acc: 0.9640\n",
      "Epoch 324/500\n",
      " - 5s - loss: 0.1424 - acc: 0.9590\n",
      "Epoch 325/500\n",
      " - 5s - loss: 0.1423 - acc: 0.9660\n",
      "Epoch 326/500\n",
      " - 6s - loss: 0.1434 - acc: 0.9600\n",
      "Epoch 327/500\n",
      " - 6s - loss: 0.1430 - acc: 0.9630\n",
      "Epoch 328/500\n",
      " - 6s - loss: 0.1444 - acc: 0.9660\n",
      "Epoch 329/500\n",
      " - 6s - loss: 0.1432 - acc: 0.9620\n",
      "Epoch 330/500\n",
      " - 5s - loss: 0.2730 - acc: 0.9290\n",
      "Epoch 331/500\n",
      " - 5s - loss: 0.1342 - acc: 0.9750\n",
      "Epoch 332/500\n",
      " - 5s - loss: 0.1334 - acc: 0.9690\n",
      "Epoch 333/500\n",
      " - 5s - loss: 0.1354 - acc: 0.9700\n",
      "Epoch 334/500\n",
      " - 5s - loss: 0.1376 - acc: 0.9670\n",
      "Epoch 335/500\n",
      " - 6s - loss: 0.1368 - acc: 0.9630\n",
      "Epoch 336/500\n",
      " - 6s - loss: 0.1391 - acc: 0.9650\n",
      "Epoch 337/500\n",
      " - 5s - loss: 0.1647 - acc: 0.9590\n",
      "Epoch 338/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 6s - loss: 0.1383 - acc: 0.9690\n",
      "Epoch 339/500\n",
      " - 6s - loss: 0.2042 - acc: 0.9430\n",
      "Epoch 340/500\n",
      " - 5s - loss: 0.1551 - acc: 0.9720\n",
      "Epoch 341/500\n",
      " - 6s - loss: 0.1292 - acc: 0.9690\n",
      "Epoch 342/500\n",
      " - 5s - loss: 0.1323 - acc: 0.9710\n",
      "Epoch 343/500\n",
      " - 6s - loss: 0.1337 - acc: 0.9670\n",
      "Epoch 344/500\n",
      " - 6s - loss: 0.1340 - acc: 0.9660\n",
      "Epoch 345/500\n",
      " - 6s - loss: 0.2425 - acc: 0.9430\n",
      "Epoch 346/500\n",
      " - 6s - loss: 0.1271 - acc: 0.9730\n",
      "Epoch 347/500\n",
      " - 6s - loss: 0.1274 - acc: 0.9640\n",
      "Epoch 348/500\n",
      " - 6s - loss: 0.1324 - acc: 0.9650\n",
      "Epoch 349/500\n",
      " - 6s - loss: 0.1311 - acc: 0.9670\n",
      "Epoch 350/500\n",
      " - 6s - loss: 0.1328 - acc: 0.9690\n",
      "Epoch 351/500\n",
      " - 6s - loss: 0.2240 - acc: 0.9470\n",
      "Epoch 352/500\n",
      " - 6s - loss: 0.1246 - acc: 0.9760\n",
      "Epoch 353/500\n",
      " - 6s - loss: 0.1273 - acc: 0.9760\n",
      "Epoch 354/500\n",
      " - 6s - loss: 0.1277 - acc: 0.9720\n",
      "Epoch 355/500\n",
      " - 6s - loss: 0.1281 - acc: 0.9690\n",
      "Epoch 356/500\n",
      " - 6s - loss: 0.1302 - acc: 0.9680\n",
      "Epoch 357/500\n",
      " - 6s - loss: 0.1302 - acc: 0.9680\n",
      "Epoch 358/500\n",
      " - 6s - loss: 0.1544 - acc: 0.9610\n",
      "Epoch 359/500\n",
      " - 6s - loss: 0.1225 - acc: 0.9700\n",
      "Epoch 360/500\n",
      " - 6s - loss: 0.1274 - acc: 0.9650\n",
      "Epoch 361/500\n",
      " - 6s - loss: 0.1971 - acc: 0.9570\n",
      "Epoch 362/500\n",
      " - 6s - loss: 0.1346 - acc: 0.9740\n",
      "Epoch 363/500\n",
      " - 6s - loss: 0.1258 - acc: 0.9750\n",
      "Epoch 364/500\n",
      " - 6s - loss: 0.1223 - acc: 0.9740\n",
      "Epoch 365/500\n",
      " - 6s - loss: 0.1245 - acc: 0.9730\n",
      "Epoch 366/500\n",
      " - 9s - loss: 0.1226 - acc: 0.9720\n",
      "Epoch 367/500\n",
      " - 6s - loss: 0.1267 - acc: 0.9710\n",
      "Epoch 368/500\n",
      " - 6s - loss: 0.2332 - acc: 0.9490\n",
      "Epoch 369/500\n",
      " - 5s - loss: 0.1362 - acc: 0.9710\n",
      "Epoch 370/500\n",
      " - 5s - loss: 0.1213 - acc: 0.9770\n",
      "Epoch 371/500\n",
      " - 5s - loss: 0.1236 - acc: 0.9730\n",
      "Epoch 372/500\n",
      " - 6s - loss: 0.1226 - acc: 0.9720\n",
      "Epoch 373/500\n",
      " - 5s - loss: 0.1235 - acc: 0.9730\n",
      "Epoch 374/500\n",
      " - 6s - loss: 0.1962 - acc: 0.9500\n",
      "Epoch 375/500\n",
      " - 5s - loss: 0.1162 - acc: 0.9750\n",
      "Epoch 376/500\n",
      " - 5s - loss: 0.1152 - acc: 0.9780\n",
      "Epoch 377/500\n",
      " - 6s - loss: 0.1175 - acc: 0.9760\n",
      "Epoch 378/500\n",
      " - 5s - loss: 0.1163 - acc: 0.9720\n",
      "Epoch 379/500\n",
      " - 5s - loss: 0.1220 - acc: 0.9710\n",
      "Epoch 380/500\n",
      " - 5s - loss: 0.1257 - acc: 0.9720\n",
      "Epoch 381/500\n",
      " - 5s - loss: 0.2468 - acc: 0.9460\n",
      "Epoch 382/500\n",
      " - 5s - loss: 0.1137 - acc: 0.9830\n",
      "Epoch 383/500\n",
      " - 5s - loss: 0.1143 - acc: 0.9800\n",
      "Epoch 384/500\n",
      " - 5s - loss: 0.1149 - acc: 0.9710\n",
      "Epoch 385/500\n",
      " - 5s - loss: 0.1154 - acc: 0.9740\n",
      "Epoch 386/500\n",
      " - 5s - loss: 0.1166 - acc: 0.9720\n",
      "Epoch 387/500\n",
      " - 5s - loss: 0.1192 - acc: 0.9690\n",
      "Epoch 388/500\n",
      " - 5s - loss: 0.1181 - acc: 0.9690\n",
      "Epoch 389/500\n",
      " - 5s - loss: 0.1944 - acc: 0.9620\n",
      "Epoch 390/500\n",
      " - 5s - loss: 0.1293 - acc: 0.9740\n",
      "Epoch 391/500\n",
      " - 5s - loss: 0.1075 - acc: 0.9870\n",
      "Epoch 392/500\n",
      " - 5s - loss: 0.1108 - acc: 0.9770\n",
      "Epoch 393/500\n",
      " - 5s - loss: 0.1120 - acc: 0.9780\n",
      "Epoch 394/500\n",
      " - 5s - loss: 0.1139 - acc: 0.9710\n",
      "Epoch 395/500\n",
      " - 5s - loss: 0.1154 - acc: 0.9700\n",
      "Epoch 396/500\n",
      " - 5s - loss: 0.1140 - acc: 0.9740\n",
      "Epoch 397/500\n",
      " - 5s - loss: 0.1112 - acc: 0.9720\n",
      "Epoch 398/500\n",
      " - 5s - loss: 0.1154 - acc: 0.9720\n",
      "Epoch 399/500\n",
      " - 5s - loss: 0.1147 - acc: 0.9720\n",
      "Epoch 400/500\n",
      " - 6s - loss: 0.2664 - acc: 0.9390\n",
      "Epoch 401/500\n",
      " - 5s - loss: 0.1040 - acc: 0.9830\n",
      "Epoch 402/500\n",
      " - 5s - loss: 0.1060 - acc: 0.9760\n",
      "Epoch 403/500\n",
      " - 5s - loss: 0.1065 - acc: 0.9820\n",
      "Epoch 404/500\n",
      " - 5s - loss: 0.1102 - acc: 0.9790\n",
      "Epoch 405/500\n",
      " - 5s - loss: 0.1097 - acc: 0.9740\n",
      "Epoch 406/500\n",
      " - 5s - loss: 0.1089 - acc: 0.9810\n",
      "Epoch 407/500\n",
      " - 5s - loss: 0.1112 - acc: 0.9760\n",
      "Epoch 408/500\n",
      " - 5s - loss: 0.1086 - acc: 0.9780\n",
      "Epoch 409/500\n",
      " - 5s - loss: 0.1091 - acc: 0.9770\n",
      "Epoch 410/500\n",
      " - 5s - loss: 0.2066 - acc: 0.9560\n",
      "Epoch 411/500\n",
      " - 5s - loss: 0.1029 - acc: 0.9770\n",
      "Epoch 412/500\n",
      " - 5s - loss: 0.1024 - acc: 0.9800\n",
      "Epoch 413/500\n",
      " - 5s - loss: 0.1049 - acc: 0.9820\n",
      "Epoch 414/500\n",
      " - 5s - loss: 0.1356 - acc: 0.9740\n",
      "Epoch 415/500\n",
      " - 5s - loss: 0.1051 - acc: 0.9790\n",
      "Epoch 416/500\n",
      " - 5s - loss: 0.1078 - acc: 0.9760\n",
      "Epoch 417/500\n",
      " - 5s - loss: 0.1048 - acc: 0.9790\n",
      "Epoch 418/500\n",
      " - 5s - loss: 0.1901 - acc: 0.9560\n",
      "Epoch 419/500\n",
      " - 5s - loss: 0.1118 - acc: 0.9860\n",
      "Epoch 420/500\n",
      " - 5s - loss: 0.1038 - acc: 0.9790\n",
      "Epoch 421/500\n",
      " - 5s - loss: 0.1017 - acc: 0.9760\n",
      "Epoch 422/500\n",
      " - 5s - loss: 0.1034 - acc: 0.9790\n",
      "Epoch 423/500\n",
      " - 5s - loss: 0.1023 - acc: 0.9750\n",
      "Epoch 424/500\n",
      " - 5s - loss: 0.1045 - acc: 0.9820\n",
      "Epoch 425/500\n",
      " - 5s - loss: 0.1052 - acc: 0.9750\n",
      "Epoch 426/500\n",
      " - 5s - loss: 0.1045 - acc: 0.9770\n",
      "Epoch 427/500\n",
      " - 5s - loss: 0.3667 - acc: 0.9400\n",
      "Epoch 428/500\n",
      " - 5s - loss: 0.0966 - acc: 0.9850\n",
      "Epoch 429/500\n",
      " - 5s - loss: 0.0984 - acc: 0.9810\n",
      "Epoch 430/500\n",
      " - 5s - loss: 0.0973 - acc: 0.9820\n",
      "Epoch 431/500\n",
      " - 5s - loss: 0.0994 - acc: 0.9780\n",
      "Epoch 432/500\n",
      " - 5s - loss: 0.1005 - acc: 0.9760\n",
      "Epoch 433/500\n",
      " - 5s - loss: 0.1016 - acc: 0.9770\n",
      "Epoch 434/500\n",
      " - 5s - loss: 0.1041 - acc: 0.9850\n",
      "Epoch 435/500\n",
      " - 5s - loss: 0.0966 - acc: 0.9790\n",
      "Epoch 436/500\n",
      " - 5s - loss: 0.1018 - acc: 0.9760\n",
      "Epoch 437/500\n",
      " - 5s - loss: 0.0968 - acc: 0.9800\n",
      "Epoch 438/500\n",
      " - 5s - loss: 0.1560 - acc: 0.9680\n",
      "Epoch 439/500\n",
      " - 5s - loss: 0.0948 - acc: 0.9810\n",
      "Epoch 440/500\n",
      " - 5s - loss: 0.0993 - acc: 0.9810\n",
      "Epoch 441/500\n",
      " - 5s - loss: 0.0991 - acc: 0.9770\n",
      "Epoch 442/500\n",
      " - 5s - loss: 0.0981 - acc: 0.9770\n",
      "Epoch 443/500\n",
      " - 5s - loss: 0.0989 - acc: 0.9810\n",
      "Epoch 444/500\n",
      " - 5s - loss: 0.0968 - acc: 0.9760\n",
      "Epoch 445/500\n",
      " - 5s - loss: 0.0981 - acc: 0.9750\n",
      "Epoch 446/500\n",
      " - 5s - loss: 0.1088 - acc: 0.9790\n",
      "Epoch 447/500\n",
      " - 5s - loss: 0.0944 - acc: 0.9810\n",
      "Epoch 448/500\n",
      " - 5s - loss: 0.0951 - acc: 0.9780\n",
      "Epoch 449/500\n",
      " - 5s - loss: 0.0959 - acc: 0.9790\n",
      "Epoch 450/500\n",
      " - 5s - loss: 0.0966 - acc: 0.9780\n",
      "Epoch 451/500\n",
      " - 5s - loss: 0.0970 - acc: 0.9740\n",
      "Epoch 452/500\n",
      " - 5s - loss: 0.2370 - acc: 0.9520\n",
      "Epoch 453/500\n",
      " - 5s - loss: 0.0882 - acc: 0.9900\n",
      "Epoch 454/500\n",
      " - 5s - loss: 0.0894 - acc: 0.9850\n",
      "Epoch 455/500\n",
      " - 6s - loss: 0.0910 - acc: 0.9870\n",
      "Epoch 456/500\n",
      " - 5s - loss: 0.0909 - acc: 0.9850\n",
      "Epoch 457/500\n",
      " - 5s - loss: 0.0919 - acc: 0.9830\n",
      "Epoch 458/500\n",
      " - 5s - loss: 0.0937 - acc: 0.9800\n",
      "Epoch 459/500\n",
      " - 5s - loss: 0.0913 - acc: 0.9790\n",
      "Epoch 460/500\n",
      " - 5s - loss: 0.0944 - acc: 0.9760\n",
      "Epoch 461/500\n",
      " - 5s - loss: 0.1621 - acc: 0.9630\n",
      "Epoch 462/500\n",
      " - 5s - loss: 0.0996 - acc: 0.9840\n",
      "Epoch 463/500\n",
      " - 5s - loss: 0.0849 - acc: 0.9930\n",
      "Epoch 464/500\n",
      " - 5s - loss: 0.0873 - acc: 0.9790\n",
      "Epoch 465/500\n",
      " - 6s - loss: 0.0933 - acc: 0.9830\n",
      "Epoch 466/500\n",
      " - 5s - loss: 0.0885 - acc: 0.9840\n",
      "Epoch 467/500\n",
      " - 5s - loss: 0.0898 - acc: 0.9810\n",
      "Epoch 468/500\n",
      " - 5s - loss: 0.0902 - acc: 0.9800\n",
      "Epoch 469/500\n",
      " - 5s - loss: 0.0904 - acc: 0.9830\n",
      "Epoch 470/500\n",
      " - 5s - loss: 0.0883 - acc: 0.9800\n",
      "Epoch 471/500\n",
      " - 5s - loss: 0.0894 - acc: 0.9800\n",
      "Epoch 472/500\n",
      " - 6s - loss: 0.1798 - acc: 0.9610\n",
      "Epoch 473/500\n",
      " - 5s - loss: 0.0991 - acc: 0.9850\n",
      "Epoch 474/500\n",
      " - 5s - loss: 0.0850 - acc: 0.9860\n",
      "Epoch 475/500\n",
      " - 5s - loss: 0.0844 - acc: 0.9870\n",
      "Epoch 476/500\n",
      " - 5s - loss: 0.0885 - acc: 0.9850\n",
      "Epoch 477/500\n",
      " - 5s - loss: 0.0859 - acc: 0.9830\n",
      "Epoch 478/500\n",
      " - 5s - loss: 0.0851 - acc: 0.9850\n",
      "Epoch 479/500\n",
      " - 5s - loss: 0.0882 - acc: 0.9790\n",
      "Epoch 480/500\n",
      " - 5s - loss: 0.0873 - acc: 0.9810\n",
      "Epoch 481/500\n",
      " - 5s - loss: 0.0880 - acc: 0.9800\n",
      "Epoch 482/500\n",
      " - 5s - loss: 0.0866 - acc: 0.9810\n",
      "Epoch 483/500\n",
      " - 5s - loss: 0.1484 - acc: 0.9730\n",
      "Epoch 484/500\n",
      " - 5s - loss: 0.1092 - acc: 0.9770\n",
      "Epoch 485/500\n",
      " - 5s - loss: 0.0808 - acc: 0.9840\n",
      "Epoch 486/500\n",
      " - 5s - loss: 0.0830 - acc: 0.9870\n",
      "Epoch 487/500\n",
      " - 5s - loss: 0.0860 - acc: 0.9780\n",
      "Epoch 488/500\n",
      " - 5s - loss: 0.0837 - acc: 0.9850\n",
      "Epoch 489/500\n",
      " - 5s - loss: 0.1275 - acc: 0.9700\n",
      "Epoch 490/500\n",
      " - 5s - loss: 0.2087 - acc: 0.9550\n",
      "Epoch 491/500\n",
      " - 5s - loss: 0.0767 - acc: 0.9910\n",
      "Epoch 492/500\n",
      " - 5s - loss: 0.0779 - acc: 0.9900\n",
      "Epoch 493/500\n",
      " - 5s - loss: 0.0790 - acc: 0.9920\n",
      "Epoch 494/500\n",
      " - 5s - loss: 0.0812 - acc: 0.9860\n",
      "Epoch 495/500\n",
      " - 5s - loss: 0.0815 - acc: 0.9870\n",
      "Epoch 496/500\n",
      " - 5s - loss: 0.0811 - acc: 0.9840\n",
      "Epoch 497/500\n",
      " - 6s - loss: 0.1914 - acc: 0.9610\n",
      "Epoch 498/500\n",
      " - 5s - loss: 0.1258 - acc: 0.9730\n",
      "Epoch 499/500\n",
      " - 5s - loss: 0.0752 - acc: 0.9940\n",
      "Epoch 500/500\n",
      " - 5s - loss: 0.0786 - acc: 0.9890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x261918d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=batch_size, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP5. 評估模型準確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 98.50%\n"
     ]
    }
   ],
   "source": [
    "# 評估模型的性能\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP6. 預測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['B', 'C'] -> D\n",
      "['G', 'H', 'I'] -> J\n",
      "['Q', 'R', 'S', 'T'] -> U\n",
      "['D', 'E', 'F'] -> G\n",
      "['I', 'J', 'K'] -> L\n",
      "['G', 'H', 'I'] -> J\n",
      "['K', 'L', 'M'] -> N\n",
      "['N'] -> O\n",
      "['A', 'B', 'C', 'D', 'E'] -> F\n",
      "['X'] -> Y\n",
      "['A', 'B', 'C', 'D'] -> E\n",
      "['V'] -> W\n",
      "['Q', 'R', 'S', 'T', 'U'] -> V\n",
      "['B', 'C', 'D', 'E', 'F'] -> G\n",
      "['R'] -> S\n",
      "['W'] -> X\n",
      "['A'] -> B\n",
      "['E', 'F', 'G'] -> H\n",
      "['T'] -> U\n"
     ]
    }
   ],
   "source": [
    "# 讓我們擷取1~5個字符轉成張量結構 shape:(1,5,1)來進行infer\n",
    "for i in range(20):\n",
    "    pattern_index = numpy.random.randint(len(dataX))\n",
    "    pattern = dataX[pattern_index]\n",
    "    x = pad_sequences([pattern], maxlen=max_len, dtype='float32')\n",
    "    x = numpy.reshape(x, (1, max_len, 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "我們可以看到，雖然這個網絡模型沒有從生成的序列資料中完全學習到英文字母表的順序，但它表現相當的好。如果需要, 我們可以對這個模型進行進一歩的優化與調整，比如更多的訓練循環(more epochs)或更大的網絡(larger network)，或兩者。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 參考:\n",
    "* Jason Brownlee - \"[Understanding Stateful LSTM Recurrent Neural Networks in Python with Keras](https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/)\"\n",
    "\n",
    "* Keras官網 - [Recurrent Layer](https://keras.io/layers/recurrent/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
